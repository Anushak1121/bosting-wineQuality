{"cells":[{"cell_type":"markdown","metadata":{"id":"4Dfc5MrE40Zv"},"source":["## Problem Statement"]},{"cell_type":"markdown","metadata":{"id":"4ceShIs03yrB"},"source":["### Context\n","\n","Wine is a beverage made from fermented grapes and other fruit juices with a low amount of alcohol content. Wine is the second most popular alcoholic drink in the world after beer, and it is one of the most highly consumed beverages.\n","\n","Generally, the quality of wine is graded based on the taste of the wine and vintage but this process is time-consuming, costly, and not efficient as the quality of the wine also depends on other physiochemical attributes like fixed acidity, volatile acidity, etc. Also, it is not always possible to ensure wine quality by experts when there is a huge demand for the product as it will increase the cost significantly."]},{"cell_type":"markdown","metadata":{"id":"PuT0tkZU3yrF"},"source":["### Objective:\n","\n","Moonshine is a red wine company that produces premium high-quality wines. The company wants to improve its production efficiency and reduce the cost and additional time involved in wine tasting. You as a data scientist at Moonshine company have to build a predictive model that can help to identify the premium quality wines using the available data."]},{"cell_type":"markdown","metadata":{"id":"hmab9zYI3yrG"},"source":["### Data Description:\n","\n","- fixed acidity: Fixed Acidity impart sourness and resist microbial infection, measured in no. of grams of tartaric acid per dm3\n","- volatile acidity: No. of grams of acetic acid per dm3 of wine. Too high levels can lead to an unpleasant, vinegar-like taste\n","- citric acid: No. of grams of citric acid per dm3 of wine. Citric acid can add freshness and flavor to wines\n","- residual sugar: Remaining sugar after fermentation stops, measured in no. of grams per dm3.\n","- chlorides: No. of grams of sodium chloride i.e. salt per dm3 of wine\n","- free sulfur dioxide: No. of grams of free sulfite per dm3 of wine\n","- total sulfur dioxide: No. of grams of total sulfite (free sulphite+ bound) per dm3 of wine\n","- density: Density in gram per cm3\n","- pH: Describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic). The pH of the wine can help measure the ripeness of the wine\n","- sulphates: No. of grams of potassium sulfate per dm3 of wine\n","- alcohol: Volume of alcohol in percentage\n","- quality: Wine quality score between 3 to 8"]},{"cell_type":"code","source":["!pip install scikit-learn==1.5"],"metadata":{"id":"d2LnH30lT7j3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sF6xhL7-3yrH"},"source":["## Import necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KpMVyeJ93yrH"},"outputs":[],"source":["# Library to suppress warnings or deprecation notes\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Libraries to help with reading and manipulating data\n","import numpy as np\n","import pandas as pd\n","\n","# Libraries to help with data visualization\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","# Libraries to split data, impute missing values\n","from sklearn.model_selection import train_test_split\n","from sklearn.impute import SimpleImputer\n","\n","# Libraries to import decision tree classifier and different ensemble classifiers\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Libtune to tune model, get different metric scores\n","from sklearn import metrics\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from sklearn.model_selection import GridSearchCV"]},{"cell_type":"markdown","metadata":{"id":"a9ZMNfUNhU14"},"source":["## Reading the dataset"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"GL4kM4eNxOSA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upzUMfBl3yrI"},"outputs":[],"source":["wine = pd.read_csv('/content/drive/MyDrive/AIMLTraining/Advanced ML/winequality-red.csv',sep=';')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-76E6S23yrJ"},"outputs":[],"source":["# copying data to another varaible to avoid any changes to original data\n","data = wine.copy()"]},{"cell_type":"markdown","metadata":{"id":"zeteOUrw5RR_"},"source":["## Overview of the dataset"]},{"cell_type":"markdown","metadata":{"id":"t6E2VnfB3yrJ"},"source":["### Displaying the first and last 5 rows of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-rGVl3qA3yrK","scrolled":true},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4T-RkZBw3yrL"},"outputs":[],"source":["data.tail()"]},{"cell_type":"markdown","metadata":{"id":"dEuSu6e4hU2e"},"source":["### Understand the shape of the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3k-NfHqE3yrL"},"outputs":[],"source":["data.shape"]},{"cell_type":"markdown","metadata":{"id":"g3SzOfEy3yrM"},"source":["* There are 1,599 observations and 12 columns in the dataset"]},{"cell_type":"markdown","metadata":{"id":"mwAu-vEwhU2m"},"source":["### Check the data types of the columns for the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"moBu6yUR3yrM"},"outputs":[],"source":["data.info()"]},{"cell_type":"markdown","metadata":{"id":"HCCVaRb03yrN"},"source":["**Observations-**\n","* All features are numeric types.\n","* There are no missing values in the data."]},{"cell_type":"markdown","metadata":{"id":"7E1orwlJhU20"},"source":["### Summary of the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsIckPuU3yrN"},"outputs":[],"source":["data.describe().T"]},{"cell_type":"markdown","metadata":{"id":"NsfoFnHr3yrN"},"source":["**Observations-**\n","* The maximum rating of wine is 8 and the third quartile value is 6 i.e. at least 75% of wines have a rating of 6 or below.\n","* There might be outliers in data where the wines have high sulfur dioxide (free sulfur dioxide and total sulfur dioxide) content as there is a big difference in the 3rd quartile and maximum values.\n","* Most wines are on a pH scale between 3 and 4."]},{"cell_type":"markdown","metadata":{"id":"kUJ_B5KxhU3D"},"source":["## Exploratory Data Analysis"]},{"cell_type":"markdown","metadata":{"id":"7Rwx-1ZuhU3D"},"source":["### Univariate analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BCAIPVz3yrO"},"outputs":[],"source":["# function to plot a boxplot and a histogram along the same scale.\n","\n","\n","def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n","    \"\"\"\n","    Boxplot and histogram combined\n","\n","    data: dataframe\n","    feature: dataframe column\n","    figsize: size of figure (default (12,7))\n","    kde: whether to show the density curve (default False)\n","    bins: number of bins for histogram (default None)\n","    \"\"\"\n","    f2, (ax_box2, ax_hist2) = plt.subplots(\n","        nrows=2,  # Number of rows of the subplot grid= 2\n","        sharex=True,  # x-axis will be shared among all subplots\n","        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n","        figsize=figsize,\n","    )  # creating the 2 subplots\n","    sns.boxplot(\n","        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n","    )  # boxplot will be created and a triangle will indicate the mean value of the column\n","    sns.histplot(\n","        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n","    ) if bins else sns.histplot(\n","        data=data, x=feature, kde=kde, ax=ax_hist2\n","    )  # For histogram\n","    ax_hist2.axvline(\n","        data[feature].mean(), color=\"green\", linestyle=\"--\"\n","    )  # Add mean to the histogram\n","    ax_hist2.axvline(\n","        data[feature].median(), color=\"black\", linestyle=\"-\"\n","    )  # Add median to the histogram"]},{"cell_type":"markdown","metadata":{"id":"FtN9V2SY3yrO"},"source":["#### Observations on fixed acidity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_XolBmSY3yrP"},"outputs":[],"source":["histogram_boxplot(data,'fixed acidity')"]},{"cell_type":"markdown","metadata":{"id":"fyTZCVVL3yrP"},"source":["* The distribution is nearly symmetric with some outliers to the right. The mean and median values are close to 8 g/dm^3\n","* The outliers present to the right indicate some wines have higher acidity than others."]},{"cell_type":"markdown","metadata":{"id":"9zdswWqj3yrP"},"source":["#### Observations on volatile acidity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SoW_Gsb93yrP"},"outputs":[],"source":["histogram_boxplot(data,'volatile acidity')"]},{"cell_type":"markdown","metadata":{"id":"K73s0-Ig3yrQ"},"source":["* Volatile acidity has a fairly normal distribution with the mean and median equal to 0.5.\n","* Some wines have a higher amount of acetic acid that would result in a vinegar-like taste of wine, possibly such wines would be rated lower in quality."]},{"cell_type":"markdown","metadata":{"id":"L75wWyWq3yrQ"},"source":["#### Observations on citric acid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GybGum703yrQ"},"outputs":[],"source":["histogram_boxplot(data,'citric acid')"]},{"cell_type":"markdown","metadata":{"id":"sm7M2VMl3yrQ"},"source":["* Citric acidity has a slightly right-skewed distribution with mean and median equal to 0.25.\n","* There is one outlier to the right with a high value of citric acid."]},{"cell_type":"markdown","metadata":{"id":"jquNwUKu3yrQ"},"source":["#### Observations on residual sugar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IbOQILx43yrQ"},"outputs":[],"source":["histogram_boxplot(data,'residual sugar')"]},{"cell_type":"markdown","metadata":{"id":"5WIv3RMS3yrR"},"source":["* The distribution of the variable is skewed to the right and there are many extreme values.\n","* Lesser residual sugar wines(< 1) are rare and we can see that majority of wines are concentrated around 1.5 - 2.5 g/dm^3 i.e. most of the wines have normal residual sugar."]},{"cell_type":"markdown","metadata":{"id":"wYpQ3ORM3yrR"},"source":["#### Observations on chlorides"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrpYpTk73yrT"},"outputs":[],"source":["histogram_boxplot(data,'chlorides')"]},{"cell_type":"markdown","metadata":{"id":"kj_2om9H3yrT"},"source":["* chlorides have a bell-shaped curve distribution.\n","* From boxplot, we can see that there are outliers on both sides of whiskers."]},{"cell_type":"markdown","metadata":{"id":"tPiMhKsk3yrT"},"source":["#### Observations on free sulfur dioxide"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g9F_8ZiF3yrT"},"outputs":[],"source":["histogram_boxplot(data,'free sulfur dioxide')"]},{"cell_type":"markdown","metadata":{"id":"dkddyWPx3yrT"},"source":["* The distribution of free sulfur dioxide is skewed to the right.\n","* From the boxplot, we can see that there are outliers - some wines have high free sulfur dioxide concentration."]},{"cell_type":"markdown","metadata":{"id":"_1T97-eF3yrT"},"source":["#### Observations on total sulfur dioxide"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pr2qXQGJ3yrT"},"outputs":[],"source":["histogram_boxplot(data,'total sulfur dioxide')"]},{"cell_type":"markdown","metadata":{"id":"omIpXroY3yrT"},"source":["* The distribution of total sulfur dioxide concentration is right-skewed.\n","* From the boxplot, we can see that there are outliers in these variables having higher total sulfur dioxide concentration than others.\n","* The two extreme values to the right can be considered as outliers. We can cap these values to the next value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUXOHJyG3yrU"},"outputs":[],"source":["#Calculating top 5 values\n","data['total sulfur dioxide'].sort_values(ascending=False).head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPkcgEDt3yrU"},"outputs":[],"source":["#Capping the two extreme values\n","data['total sulfur dioxide']=data['total sulfur dioxide'].clip(upper=165)"]},{"cell_type":"markdown","metadata":{"id":"q0hulISp3yrU"},"source":["#### Observations on density"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WP16GuY73yrU"},"outputs":[],"source":["histogram_boxplot(data,'density')"]},{"cell_type":"markdown","metadata":{"id":"7kiIn9u-3yrU"},"source":["* The Distribution of density follows a normal distribution with mean and median equal to ~0.997.\n","* From the boxplot, we can see that there are outliers on both the sides of whiskers indicating some wines are too dense and some very low density."]},{"cell_type":"markdown","metadata":{"id":"idy7IN9T3yrU"},"source":["#### Observations on pH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stiMN93S3yrU"},"outputs":[],"source":["histogram_boxplot(data,'pH')"]},{"cell_type":"markdown","metadata":{"id":"VzzdM9903yrU"},"source":["* The distribution of pH looks normally distributed with mean and median equal to ~3.3, most wines have a pH value around 3.3 i.e. most wines are acidic.\n","* From the boxplot, we can see that there are outliers on both sides of whiskers."]},{"cell_type":"markdown","metadata":{"id":"ywjsl5zj3yrV"},"source":["#### Observations on sulphates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zTDgOl6w3yrV"},"outputs":[],"source":["histogram_boxplot(data,'sulphates')"]},{"cell_type":"markdown","metadata":{"id":"Xn4LMJna3yrV"},"source":["* The distribution of sulphates, which act as antimicrobial and antioxidant in wines, is right-skewed.\n","* From the boxplot, we can see that there are outliers in this variable i.e. some wines with very high sulphates than others, can be highly rated wines as sulfates would help them in preserving them for a longer time."]},{"cell_type":"markdown","metadata":{"id":"q6V1BuXc3yrV"},"source":["#### Observations on alcohol"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5alCnAlV3yrV"},"outputs":[],"source":["histogram_boxplot(data,'alcohol')"]},{"cell_type":"markdown","metadata":{"id":"rw4yWMonGh_r"},"source":["#### Observations on Quality"]},{"cell_type":"markdown","metadata":{"id":"Q9qulKS03yrV"},"source":["* Most wines have 9% to 10% of alcohol.\n","* From the boxplot, we can see that there are some extreme values for wines having more than 13% of alcohol."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZFFXoDt3yrV"},"outputs":[],"source":["# function to create labeled barplots\n","\n","\n","def labeled_barplot(data, feature, perc=False, n=None):\n","    \"\"\"\n","    Barplot with percentage at the top\n","\n","    data: dataframe\n","    feature: dataframe column\n","    perc: whether to display percentages instead of count (default is False)\n","    n: displays the top n category levels (default is None, i.e., display all levels)\n","    \"\"\"\n","\n","    total = len(data[feature])  # length of the column\n","    count = data[feature].nunique()\n","    if n is None:\n","        plt.figure(figsize=(count + 1, 5))\n","    else:\n","        plt.figure(figsize=(n + 1, 5))\n","\n","    plt.xticks(rotation=90, fontsize=15)\n","    ax = sns.countplot(\n","        data=data,\n","        x=feature,\n","        palette=\"Paired\",\n","        order=data[feature].value_counts().index[:n],\n","    )\n","\n","    for p in ax.patches:\n","        if perc == True:\n","            label = \"{:.1f}%\".format(\n","                100 * p.get_height() / total\n","            )  # percentage of each class of the category\n","        else:\n","            label = p.get_height()  # count of each level of the category\n","\n","        x = p.get_x() + p.get_width() / 2  # width of the plot\n","        y = p.get_height()  # height of the plot\n","\n","        ax.annotate(\n","            label,\n","            (x, y),\n","            ha=\"center\",\n","            va=\"center\",\n","            size=12,\n","            xytext=(0, 5),\n","            textcoords=\"offset points\",\n","        )  # annotate the percentage\n","\n","    plt.show()  # show the plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C10QD6ao3yrV"},"outputs":[],"source":["labeled_barplot(data,\"quality\",perc=True)"]},{"cell_type":"markdown","metadata":{"id":"_Smu7iCY3yrV"},"source":["* Most of the wines are rated either 5 or 6 and there are very few records for 3, 4, and 8 rated wines.\n","* The observations with 7 or 8 are very few. This might be because since these are high-quality premium wines with maximum rating.\n","* We can combine records to create two broad categories - premium quality wines and non-premium quality wines, where quality:  2-6 can be one class and quality: 7-8 can be the other class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"if-KJKTB3yrW"},"outputs":[],"source":["# defining bins\n","bins = (2, 6, 8)\n","# defining labels\n","labels = ['non-premium', 'premium']\n","\n","data['quality_class'] = pd.cut(x = data['quality'], bins = bins, labels = labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKAqJzJ-3yrW"},"outputs":[],"source":["data['quality_class'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"--Ga3h003yrW"},"source":["* We have reduced the number of categories to only two categories.\n","* The classes are imbalanced as there are only 217 observations with the premium class."]},{"cell_type":"markdown","metadata":{"id":"beo_tDmVhU3-"},"source":["### Bivariate Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8LrBMcN43yrW"},"outputs":[],"source":["plt.figure(figsize=(10,7))\n","sns.heatmap(data.drop('quality_class', axis=1).corr(),annot=True,vmin=-1,vmax=1,fmt='.1g',cmap=\"Spectral\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"sxD6H0sZ3yrW"},"source":["* Fixed acidity has a strong positive correlation with citric acid and density.\n","* The total sulfur dioxide and free sulfur dioxide have a strong correlation.\n","* The quality of wine shows a moderate correlation with alcohol."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNY2WQgk3yrW","scrolled":true},"outputs":[],"source":["sns.pairplot(data,hue='quality_class')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-aEYW7Kr3yrW"},"source":["* There is overlap in the distribution of variables for both classes. Let's explore this further using other plots."]},{"cell_type":"markdown","metadata":{"id":"9oVGsi3o3yrW"},"source":["#### Quality vs acidity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKqjBFj-3yrW"},"outputs":[],"source":["cols = data[['fixed acidity', 'volatile acidity', 'citric acid']].columns.tolist()\n","plt.figure(figsize=(12,5))\n","\n","for i, variable in enumerate(cols):\n","                     plt.subplot(1,3,i+1)\n","                     sns.boxplot(data=data,x='quality_class',y=variable,palette=\"PuBu\")\n","                     plt.tight_layout()\n","                     plt.title(variable)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"7Kv54scY3yrX"},"source":["* Premium wines have higher fixed acidity and citric acid and lower volatile acidity."]},{"cell_type":"markdown","metadata":{"id":"VPntxm3-3yrX"},"source":["#### Quality vs sulfur"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"usBu754y3yrX"},"outputs":[],"source":["cols = data[['free sulfur dioxide', 'total sulfur dioxide', 'sulphates']].columns.tolist()\n","plt.figure(figsize=(12,5))\n","\n","for i, variable in enumerate(cols):\n","                     plt.subplot(1,3,i+1)\n","                     sns.boxplot(data=data,x='quality_class',y=variable,palette=\"PuBu\")\n","                     plt.tight_layout()\n","                     plt.title(variable)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HkiYKQ1s3yrX"},"source":["* Premium wines have less concentration of free sulfur dioxide and total sulfur dioxide but higher sulfates."]},{"cell_type":"markdown","metadata":{"id":"obaFp2US3yrX"},"source":["#### Quality vs chlorides"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p1IQJ8b83yrX"},"outputs":[],"source":["## function to plot boxplots w.rt quality\n","def boxplot(x):\n","    plt.figure(figsize=(7,5))\n","    sns.boxplot(data=data,x=x,y=data['quality_class'],palette=\"PuBu\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rV_ossmP3yrX"},"outputs":[],"source":["boxplot(data['chlorides'])"]},{"cell_type":"markdown","metadata":{"id":"8XfA7pg-3yrX"},"source":["* There's not much difference between the chlorides for both classes.\n","* There are many outliers for the non-premium wines. It is quite difficult to interpret here, let's turn off the outliers and visualize again."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlOFyiOC3yrX"},"outputs":[],"source":["sns.boxplot(data=data,x='quality_class',y='chlorides',showfliers=False,palette='PuBu');"]},{"cell_type":"markdown","metadata":{"id":"8WRzPK0j3yrX"},"source":["* Premium wines have a lower concentration of chlorides in them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYMrTTQj3yrY"},"outputs":[],"source":["boxplot(data['density'])"]},{"cell_type":"markdown","metadata":{"id":"j_GNqFWS3yrY"},"source":["* Premium wines have lesser density i.e. they have comparatively thinner than the non-premium wines."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQAsfK683yrY"},"outputs":[],"source":["boxplot(data['pH'])"]},{"cell_type":"markdown","metadata":{"id":"4tj0YA5i3yrY"},"source":["* There is not much difference between the pH value of both types of wines but the range of pH value of non-premium wines is higher than the range of pH value of premium wines. This indicates that some non-premium wines are either too acidic or less acidic."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rf9ICn4-3yrY"},"outputs":[],"source":["boxplot(data['residual sugar'])"]},{"cell_type":"markdown","metadata":{"id":"QHqAvi8p3yrY"},"source":["* As we observed earlier some wines have high residual sugar, such wines are rated lower."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SSyaHSl3yrY","scrolled":true},"outputs":[],"source":["boxplot(data['alcohol'])"]},{"cell_type":"markdown","metadata":{"id":"ggGMEHNx3yrY"},"source":["* Alcohol content plays a huge role in wine quality. We can see that premium wines have a higher alcohol content as compared to non-premium wines."]},{"cell_type":"markdown","metadata":{"id":"yV1E3sgB3yrY"},"source":["## Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"qb6R8u1X6noN"},"source":["### Data Preparation for Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w0Wvopux3yrZ"},"outputs":[],"source":["data.drop('quality', axis=1, inplace=True)\n","X = data.drop('quality_class', axis=1)\n","y = data['quality_class'].apply(lambda x : 0 if x=='non-premium' else 1 )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqVHLEHVRRKK"},"outputs":[],"source":["# Splitting data into training and test set:\n","X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)\n","print(X_train.shape, X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"bcEwPN7U3yrZ"},"source":["**Note**: The stratify argument maintains the original distribution of classes in the target variable while splitting the data into train and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2SmH_F03yrZ"},"outputs":[],"source":["y.value_counts(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGyn2LJo3yrZ"},"outputs":[],"source":["y_test.value_counts(1)"]},{"cell_type":"markdown","metadata":{"id":"hiK8ipUE70Zl"},"source":["## Model evaluation criterion"]},{"cell_type":"markdown","metadata":{"id":"Nf0J3jEG3yrZ"},"source":["\n","**The model can make wrong predictions as:**\n","1. Predicting a wine is of premium quality when it is of non-premium quality.\n","2. Predicting a wine is of non-premium quality when it is of premium quality.\n","\n","**Which case is more important? **\n","1. If the model predicts a wine is of non-premium quality but it is of premium quality then the company would incur the loss of good wine and resources used.\n","2. If the model predicts a wine is of premium quality but it is not then the company would roll out low-quality wine which would affect their customer base and their reputation.\n","\n","f1 score = 2*precision*recall / (precision +recall)\n","\n","**Which metric to optimize?**\n","* We would want F1-Score to be maximized, the greater the F1-Score higher the chances of predicting both the classes correctly."]},{"cell_type":"markdown","metadata":{"id":"J4NWLDzi3yrZ"},"source":["**Let's define a function to provide metric scores on the train and test set and a function to show confusion matrix so that we do not have to use the same code repetitively while evaluating models.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_ZZq1L23yra"},"outputs":[],"source":["# defining a function to compute different metrics to check performance of a classification model built using sklearn\n","def model_performance_classification_sklearn(model, predictors, target):\n","    \"\"\"\n","    Function to compute different metrics to check classification model performance\n","\n","    model: classifier\n","    predictors: independent variables\n","    target: dependent variable\n","    \"\"\"\n","\n","    # predicting using the independent variables\n","    pred = model.predict(predictors)\n","\n","    acc = accuracy_score(target, pred)  # to compute Accuracy\n","    recall = recall_score(target, pred)  # to compute Recall\n","    precision = precision_score(target, pred)  # to compute Precision\n","    f1 = f1_score(target, pred)  # to compute F1-score\n","\n","    # creating a dataframe of metrics\n","    df_perf = pd.DataFrame(\n","        {\n","            \"Accuracy\": acc,\n","            \"Recall\": recall,\n","            \"Precision\": precision,\n","            \"F1\": f1,\n","        },\n","        index=[0],\n","    )\n","\n","    return df_perf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TxaWEPFX3yra"},"outputs":[],"source":["def confusion_matrix_sklearn(model, predictors, target):\n","    \"\"\"\n","    To plot the confusion_matrix with percentages\n","\n","    model: classifier\n","    predictors: independent variables\n","    target: dependent variable\n","    \"\"\"\n","    y_pred = model.predict(predictors)\n","    cm = confusion_matrix(target, y_pred)\n","    labels = np.asarray(\n","        [\n","            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n","            for item in cm.flatten()\n","        ]\n","    ).reshape(2, 2)\n","\n","    plt.figure(figsize=(6, 4))\n","    sns.heatmap(cm, annot=labels, fmt=\"\")\n","    plt.ylabel(\"True label\")\n","    plt.xlabel(\"Predicted label\")"]},{"cell_type":"markdown","metadata":{"id":"U_imjVzRxLQC"},"source":["## Decision Tree - Model Building and Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{"id":"hKrCxAt6TCo-"},"source":["### Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYsH6SEbR2Zv","scrolled":false},"outputs":[],"source":["#Fitting the model\n","d_tree = DecisionTreeClassifier(random_state=1)\n","d_tree.fit(X_train,y_train)\n","\n","#Calculating different metrics\n","d_tree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)\n","print(\"Training performance:\\n\",d_tree_model_train_perf)\n","d_tree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)\n","print(\"Testing performance:\\n\",d_tree_model_test_perf)\n","\n","#Creating confusion matrix\n","confusion_matrix_sklearn(d_tree,X_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"Fq-I_vIN3yra"},"source":["* The decision tree is overfitting the training data.\n","* Let's try hyperparameter tuning and see if the model performance improves."]},{"cell_type":"markdown","metadata":{"id":"6-P4chAm3yra"},"source":["### Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SakrLtI73yra"},"outputs":[],"source":["#Choose the type of classifier.\n","dtree_estimator = DecisionTreeClassifier(class_weight={0:0.18,1:0.72},random_state=1)\n","\n","# Grid of parameters to choose from\n","parameters = {'max_depth': np.arange(2,30),\n","              'min_samples_leaf': [1, 2, 5, 7, 10],\n","              'max_leaf_nodes' : [2, 3, 5, 10,15],\n","              'min_impurity_decrease': [0.0001,0.001,0.01,0.1]\n","             }\n","\n","# Type of scoring used to compare parameter combinations\n","scorer = metrics.make_scorer(metrics.f1_score)\n","\n","# Run the grid search\n","grid_obj = GridSearchCV(dtree_estimator, parameters, scoring=scorer,n_jobs=-1)\n","grid_obj = grid_obj.fit(X_train, y_train)\n","\n","# Set the clf to the best combination of parameters\n","dtree_estimator = grid_obj.best_estimator_\n","\n","# Fit the best algorithm to the data.\n","dtree_estimator.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wLSWlusI3yrb"},"outputs":[],"source":["#Calculating different metrics\n","dtree_estimator_model_train_perf=model_performance_classification_sklearn(dtree_estimator,X_train,y_train)\n","print(\"Training performance:\\n\",dtree_estimator_model_train_perf)\n","dtree_estimator_model_test_perf=model_performance_classification_sklearn(dtree_estimator,X_test,y_test)\n","print(\"Testing performance:\\n\",dtree_estimator_model_test_perf)\n","\n","#Creating confusion matrix\n","confusion_matrix_sklearn(dtree_estimator,X_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"3cJYkQJ73yrb"},"source":["* The overfitting has reduced but the test f1-score has also decreased.\n","* Let's try some other models."]},{"cell_type":"markdown","metadata":{"id":"AgyA1X84IKP_"},"source":["## Bagging - Model Building and Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{"id":"I1bbvWme3yrb"},"source":["### Random Forest Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YFv5NKox3yrb"},"outputs":[],"source":["#Fitting the model\n","rf_estimator = RandomForestClassifier(random_state=1)\n","rf_estimator.fit(X_train,y_train)\n","\n","#Calculating different metrics\n","rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)\n","print(\"Training performance:\\n\",rf_estimator_model_train_perf)\n","rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)\n","print(\"Testing performance:\\n\",rf_estimator_model_test_perf)\n","\n","#Creating confusion matrix\n","confusion_matrix_sklearn(rf_estimator,X_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"8P_btjpz3yrb"},"source":["* Random forest is giving a slightly higher test f1-score than decision trees but it is overfitting the training data.\n","* Let's try hyperparameter tuning and see if the model performance improves."]},{"cell_type":"markdown","metadata":{"id":"0pE-aDke3yrb"},"source":["### Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TCs5ryd73yrc"},"outputs":[],"source":["# Choose the type of classifier.\n","rf_tuned = RandomForestClassifier(class_weight={0:0.18,1:0.82},random_state=1,oob_score=True,bootstrap=True)\n","\n","parameters = {\n","                'max_depth': list(np.arange(5,30,5)) + [None],\n","                'max_features': ['sqrt','log2',None],\n","                'min_samples_leaf': np.arange(1,15,5),\n","                'min_samples_split': np.arange(2, 20, 5),\n","                'n_estimators': np.arange(10,110,10)}\n","\n","\n","# Type of scoring used to compare parameter combinations\n","scorer = metrics.make_scorer(metrics.f1_score)\n","\n","# Run the grid search\n","grid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer, cv=5,n_jobs=-1)\n","grid_obj = grid_obj.fit(X_train, y_train)\n","\n","# Set the clf to the best combination of parameters\n","rf_tuned = grid_obj.best_estimator_\n","\n","# Fit the best algorithm to the data.\n","rf_tuned.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vXxfTh3R3yrc"},"outputs":[],"source":["#Calculating different metrics\n","rf_tuned_model_train_perf=model_performance_classification_sklearn(rf_tuned,X_train,y_train)\n","print(\"Training performance:\\n\",rf_tuned_model_train_perf)\n","rf_tuned_model_test_perf=model_performance_classification_sklearn(rf_tuned,X_test,y_test)\n","print(\"Testing performance:\\n\",rf_tuned_model_test_perf)\n","\n","#Creating confusion matrix\n","confusion_matrix_sklearn(rf_tuned,X_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"MWDC-cej3yrc"},"source":["* The overfitting has reduced significantly and the model performance has improved.\n","* The test recall and test f1-score have increased."]},{"cell_type":"markdown","metadata":{"id":"PDX0HcpC3yrc"},"source":["### Bagging Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FceeTEET3yrc"},"outputs":[],"source":["#Fitting the model\n","bagging_classifier = BaggingClassifier(random_state=1)\n","bagging_classifier.fit(X_train,y_train)\n","\n","#Calculating different metrics\n","bagging_classifier_model_train_perf=model_performance_classification_sklearn(bagging_classifier,X_train,y_train)\n","print(bagging_classifier_model_train_perf)\n","bagging_classifier_model_test_perf=model_performance_classification_sklearn(bagging_classifier,X_test,y_test)\n","print(bagging_classifier_model_test_perf)\n","\n","#Creating confusion matrix\n","confusion_matrix_sklearn(bagging_classifier,X_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"nBKeRk5v3yrd"},"source":["* Bagging classifier is overfitting the training data.\n","* Let's try hyperparameter tuning and see if the model performance improves."]},{"cell_type":"markdown","metadata":{"id":"m8woS6Y13yre"},"source":["### Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZ56X7TW3yre"},"outputs":[],"source":["# Choose the type of classifier.\n","bagging_estimator_tuned = BaggingClassifier(random_state=1)\n","\n","# Grid of parameters to choose from\n","parameters = {'max_samples': [0.7,0.8,0.9,1],\n","              'max_features': [0.7,0.8,0.9,1],\n","              'n_estimators' : [10,20,30,40,50],\n","             }\n","\n","# Type of scoring used to compare parameter combinations\n","scorer = metrics.make_scorer(metrics.f1_score)\n","\n","# Run the grid search\n","grid_obj = GridSearchCV(bagging_estimator_tuned, parameters, scoring=scorer,cv=5)\n","grid_obj = grid_obj.fit(X_train, y_train)\n","\n","# Set the clf to the best combination of parameters\n","bagging_estimator_tuned = grid_obj.best_estimator_\n","\n","# Fit the best algorithm to the data.\n","bagging_estimator_tuned.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PbXf0cyr3yrf"},"outputs":[],"source":["#Calculating different metrics\n","bagging_estimator_tuned_model_train_perf=model_performance_classification_sklearn(bagging_estimator_tuned,X_train,y_train)\n","print(bagging_estimator_tuned_model_train_perf)\n","bagging_estimator_tuned_model_test_perf=model_performance_classification_sklearn(bagging_estimator_tuned,X_test,y_test)\n","print(bagging_estimator_tuned_model_test_perf)\n","\n","#Creating confusion matrix\n","confusion_matrix_sklearn(bagging_estimator_tuned,X_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"CKTHHyu13yrf"},"source":["* Surprisingly, the model performance has decreased after hyperparameter tuning.\n","* Let's try now try boosting models."]},{"cell_type":"markdown","metadata":{"id":"db5ptRJTIthL"},"source":["## Boosting - Model Building and Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{"id":"MuyQOp4r3yrf"},"source":["### AdaBoost Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8vIES47GxLQs"},"outputs":[],"source":["#Fitting the model\n","ab_classifier = AdaBoostClassifier(random_state=1)\n","ab_classifier.fit(X_train,y_train)\n","\n","#Calculating different metrics\n","ab_classifier_model_train_perf=model_performance_classification_sklearn(ab_classifier,X_train,y_train)\n","print(ab_classifier_model_train_perf)\n","ab_classifier_model_test_perf=model_performance_classification_sklearn(ab_classifier,X_test,y_test)\n","print(ab_classifier_model_test_perf)\n","\n","#Creating confusion matrix\n","confusion_matrix_sklearn(ab_classifier,X_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"AxG2K1jo3yrf"},"source":["* Adaboost is giving more generalized performance than previous models but the test f1-score is too low."]},{"cell_type":"markdown","metadata":{"id":"Z8YVzIRG3yrf"},"source":["### Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rfmB9htu3yrg"},"outputs":[],"source":["# Choose the type of classifier.\n","abc_tuned = AdaBoostClassifier(random_state=1)\n","\n","# Grid of parameters to choose from\n","parameters = {\n","    #Let's try different max_depth for base_estimator\n","    \"base_estimator\":[DecisionTreeClassifier(max_depth=1),DecisionTreeClassifier(max_depth=2),\n","                      DecisionTreeClassifier(max_depth=3)],\n","    \"n_estimators\": np.arange(10,110,10),\n","    \"learning_rate\":np.arange(0.1,2,0.1)\n","}\n","\n","# Type of scoring used to compare parameter  combinations\n","scorer = metrics.make_scorer(metrics.f1_score)\n","\n","# Run the grid search\n","grid_obj = GridSearchCV(abc_tuned, parameters, scoring=scorer,cv=5)\n","grid_obj = grid_obj.fit(X_train, y_train)\n","\n","# Set the clf to the best combination of parameters\n","abc_tuned = grid_obj.best_estimator_\n","\n","# Fit the best algorithm to the data.\n","abc_tuned.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhrdSkBOxLQz"},"outputs":[],"source":["#Calculating different metrics\n","abc_tuned_model_train_perf=model_performance_classification_sklearn(abc_tuned,X_train,y_train)\n","print(abc_tuned_model_train_perf)\n","abc_tuned_model_test_perf=model_performance_classification_sklearn(abc_tuned,X_test,y_test)\n","print(abc_tuned_model_test_perf)\n","\n","#Creating confusion matrix\n","confusion_matrix_sklearn(abc_tuned,X_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"s5-l1q2X3yrg"},"source":["* The model performance has increased slightly but the model has started to overfit the training data."]},{"cell_type":"markdown","metadata":{"id":"M0_MQx2B3yrg"},"source":["### Gradient Boosting Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LOOyVUB-3yrg"},"outputs":[],"source":["#Fitting the model\n","gb_classifier = GradientBoostingClassifier(random_state=1)\n","gb_classifier.fit(X_train,y_train)\n","\n","#Calculating different metrics\n","gb_classifier_model_train_perf=model_performance_classification_sklearn(gb_classifier,X_train,y_train)\n","print(\"Training performance:\\n\",gb_classifier_model_train_perf)\n","gb_classifier_model_test_perf=model_performance_classification_sklearn(gb_classifier,X_test,y_test)\n","print(\"Testing performance:\\n\",gb_classifier_model_test_perf)\n","\n","#Creating confusion matrix\n","confusion_matrix_sklearn(gb_classifier,X_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"IKdhV6-T3yrg"},"source":["* The gradient boosting classifier is overfitting the training data."]},{"cell_type":"markdown","metadata":{"id":"4Nvg_uCx3yrh"},"source":["### Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uj7apCsW3yrh"},"outputs":[],"source":["# Choose the type of classifier.\n","gbc_tuned = GradientBoostingClassifier(init=AdaBoostClassifier(random_state=1),random_state=1)\n","\n","# Grid of parameters to choose from\n","parameters = {\n","    \"n_estimators\": [100,150,200,250],\n","    \"subsample\":[0.8,0.9,1],\n","    \"max_features\":[0.7,0.8,0.9,1]\n","}\n","\n","# Type of scoring used to compare parameter combinations\n","scorer = metrics.make_scorer(metrics.f1_score)\n","\n","# Run the grid search\n","grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=scorer,cv=5)\n","grid_obj = grid_obj.fit(X_train, y_train)\n","\n","# Set the clf to the best combination of parameters\n","gbc_tuned = grid_obj.best_estimator_\n","\n","# Fit the best algorithm to the data.\n","gbc_tuned.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rq50RM1y3yrh"},"outputs":[],"source":["#Calculating different metrics\n","gbc_tuned_model_train_perf=model_performance_classification_sklearn(gbc_tuned,X_train,y_train)\n","print(\"Training performance:\\n\",gbc_tuned_model_train_perf)\n","gbc_tuned_model_test_perf=model_performance_classification_sklearn(gbc_tuned,X_test,y_test)\n","print(\"Testing performance:\\n\",gbc_tuned_model_test_perf)\n","\n","#Creating confusion matrix\n","confusion_matrix_sklearn(gbc_tuned,X_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"8UseXyFe3yrh"},"source":["* There is not much difference in the model performance after hyperparameter tuning."]},{"cell_type":"markdown","metadata":{"id":"j8M-mGNw3yrh"},"source":["### XGBoost Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3b7fcbc63yrh"},"outputs":[],"source":["#Fitting the model\n","xgb_classifier = XGBClassifier(random_state=1, eval_metric='logloss')\n","xgb_classifier.fit(X_train,y_train)\n","\n","#Calculating different metrics\n","xgb_classifier_model_train_perf=model_performance_classification_sklearn(xgb_classifier,X_train,y_train)\n","print(\"Training performance:\\n\",xgb_classifier_model_train_perf)\n","xgb_classifier_model_test_perf=model_performance_classification_sklearn(xgb_classifier,X_test,y_test)\n","print(\"Testing performance:\\n\",xgb_classifier_model_test_perf)\n","\n","#Creating confusion matrix\n","confusion_matrix_sklearn(xgb_classifier,X_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"-BXS4Zdh3yrh"},"source":["* xgboost classifier is overfitting the training data.\n","* Let's try hyperparameter tuning and see if the model performance improves.  "]},{"cell_type":"markdown","metadata":{"id":"aX84Z81W3yri"},"source":["### Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yB3t3Xh3yri"},"outputs":[],"source":["# Choose the type of classifier.\n","xgb_tuned = XGBClassifier(random_state=1, eval_metric='logloss')\n","\n","# Grid of parameters to choose from\n","parameters = {\n","    \"n_estimators\": [10,30,50],\n","    \"scale_pos_weight\":[1,2,5],\n","    \"subsample\":[0.7,0.9,1],\n","    \"learning_rate\":[0.05, 0.1,0.2],\n","    \"colsample_bytree\":[0.7,0.9,1],\n","    \"colsample_bylevel\":[0.5,0.7,1]\n","}\n","\n","# Type of scoring used to compare parameter combinations\n","scorer = metrics.make_scorer(metrics.f1_score)\n","\n","# Run the grid search\n","grid_obj = GridSearchCV(xgb_tuned, parameters,scoring=scorer,cv=5)\n","grid_obj = grid_obj.fit(X_train, y_train)\n","\n","# Set the clf to the best combination of parameters\n","xgb_tuned = grid_obj.best_estimator_\n","\n","# Fit the best algorithm to the data.\n","xgb_tuned.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohQ2hzrA3yri"},"outputs":[],"source":["#Calculating different metrics\n","xgb_tuned_model_train_perf=model_performance_classification_sklearn(xgb_tuned,X_train,y_train)\n","print(\"Training performance:\\n\",xgb_tuned_model_train_perf)\n","xgb_tuned_model_test_perf=model_performance_classification_sklearn(xgb_tuned,X_test,y_test)\n","print(\"Testing performance:\\n\",xgb_tuned_model_test_perf)\n","\n","#Creating confusion matrix\n","confusion_matrix_sklearn(xgb_tuned,X_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"xysSl1k63yri"},"source":["* The overfitting has reduced slightly but there is not much difference in the model performance."]},{"cell_type":"markdown","metadata":{"id":"48r126vd3yri"},"source":["## Stacking Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1BUaZLM3yri"},"outputs":[],"source":["estimators = [('Random Forest',rf_tuned), ('Gradient Boosting',gbc_tuned), ('Decision Tree',dtree_estimator)]\n","\n","final_estimator = xgb_tuned\n","\n","stacking_classifier= StackingClassifier(estimators=estimators,final_estimator=final_estimator)\n","\n","stacking_classifier.fit(X_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b30hGq6I3yri"},"outputs":[],"source":["#Calculating different metrics\n","stacking_classifier_model_train_perf=model_performance_classification_sklearn(stacking_classifier,X_train,y_train)\n","print(\"Training performance:\\n\",stacking_classifier_model_train_perf)\n","stacking_classifier_model_test_perf=model_performance_classification_sklearn(stacking_classifier,X_test,y_test)\n","print(\"Testing performance:\\n\",stacking_classifier_model_test_perf)\n","\n","#Creating confusion matrix\n","confusion_matrix_sklearn(stacking_classifier,X_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"xNWz5Fl_3yri"},"source":["* The stacking classifier is giving a similar performance as compared to XGBoost with slightly less overfitting.\n","* The confusion matrix shows that the model can identify the majority of premium quality wines but it is better at identifying non-premium quality wines."]},{"cell_type":"markdown","metadata":{"id":"0NqiM3FW3yri"},"source":["## Comparing all models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSG24QlJ3yrj"},"outputs":[],"source":["# training performance comparison\n","\n","models_train_comp_df = pd.concat(\n","    [d_tree_model_train_perf.T,dtree_estimator_model_train_perf.T,rf_estimator_model_train_perf.T,rf_tuned_model_train_perf.T,\n","     bagging_classifier_model_train_perf.T,bagging_estimator_tuned_model_train_perf.T,ab_classifier_model_train_perf.T,\n","     abc_tuned_model_train_perf.T,gb_classifier_model_train_perf.T,gbc_tuned_model_train_perf.T,xgb_classifier_model_train_perf.T,\n","    xgb_tuned_model_train_perf.T,stacking_classifier_model_train_perf.T],\n","    axis=1,\n",")\n","models_train_comp_df.columns = [\n","    \"Decision Tree\",\n","    \"Decision Tree Estimator\",\n","    \"Random Forest Estimator\",\n","    \"Random Forest Tuned\",\n","    \"Bagging Classifier\",\n","    \"Bagging Estimator Tuned\",\n","    \"Adaboost Classifier\",\n","    \"Adabosst Classifier Tuned\",\n","    \"Gradient Boost Classifier\",\n","    \"Gradient Boost Classifier Tuned\",\n","    \"XGBoost Classifier\",\n","    \"XGBoost Classifier Tuned\",\n","    \"Stacking Classifier\"]\n","print(\"Training performance comparison:\")\n","models_train_comp_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7zZHyAF3yrj"},"outputs":[],"source":["# testing performance comparison\n","\n","models_test_comp_df = pd.concat(\n","    [d_tree_model_test_perf.T,dtree_estimator_model_test_perf.T,rf_estimator_model_test_perf.T,rf_tuned_model_test_perf.T,\n","     bagging_classifier_model_test_perf.T,bagging_estimator_tuned_model_test_perf.T,ab_classifier_model_test_perf.T,\n","     abc_tuned_model_test_perf.T,gb_classifier_model_test_perf.T,gbc_tuned_model_test_perf.T,xgb_classifier_model_test_perf.T,\n","    xgb_tuned_model_test_perf.T,stacking_classifier_model_test_perf.T],\n","    axis=1,\n",")\n","models_test_comp_df.columns = [\n","    \"Decision Tree\",\n","    \"Decision Tree Estimator\",\n","    \"Random Forest Estimator\",\n","    \"Random Forest Tuned\",\n","    \"Bagging Classifier\",\n","    \"Bagging Estimator Tuned\",\n","    \"Adaboost Classifier\",\n","    \"Adabosst Classifier Tuned\",\n","    \"Gradient Boost Classifier\",\n","    \"Gradient Boost Classifier Tuned\",\n","    \"XGBoost Classifier\",\n","    \"XGBoost Classifier Tuned\",\n","    \"Stacking Classifier\"]\n","print(\"Testing performance comparison:\")\n","models_test_comp_df"]},{"cell_type":"markdown","metadata":{"id":"iisvmiLW3yrj"},"source":["* The majority of the models are overfitting the training data in terms of f1-score.\n","* The bagging classifier is giving the highest f1-score on the test data but is overfitting the training data.\n","* The tuned random forest has given the second-highest test f1-score and is giving a more generalized performance as compared to the bagging classifier."]},{"cell_type":"markdown","metadata":{"id":"-XnPSDxz3yrj"},"source":["### Feature importance of Tuned Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I18J-Mge3yrj"},"outputs":[],"source":["feature_names = X_train.columns\n","importances = rf_tuned.feature_importances_\n","indices = np.argsort(importances)\n","\n","plt.figure(figsize=(12,12))\n","plt.title('Feature Importances')\n","plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n","plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n","plt.xlabel('Relative Importance')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"XCOzmyrv3yrj"},"source":["* Alcohol is the most important feature in identifying premium quality wine followed by sulfates and volatile acidity."]},{"cell_type":"markdown","metadata":{"id":"GSicJOwFEcs-"},"source":["## Conclusions and Recommendations"]},{"cell_type":"markdown","metadata":{"id":"m8DjLACb3yrj"},"source":["- Based on our analysis, we can say that the premium quality wine has the following features in comparison to the non-premium quality wine:\n","    - Higher concentration of alcohol.\n","    - A fair and higher concentration of sulfates. Fair indicates a smaller range of values or less extreme values.\n","    - Less volatile acidity.\n","    - Higher fixed acidity.\n","    - Higher citric acid concentration.\n","    - A fair and lower concentration of total sulfur dioxide and free sulfur dioxide.\n","    - Lower concentration of chlorides.\n","    - Lesser density.\n","    - A fair pH level i.e. neither very acidic nor very less acidic.\n","- The company should be more precise with the concentration and level of alcohol, volatile acidity, and sulfates as these are the most important factors in determining the quality of wines. For example, the common alcohol level for premium quality wine should be between 11-13%.\n","- Once the desired performance is achieved from the model, the company can use it to identify the premium quality wines for new production. This would help to reduce the cost and increase the efficiency of the process."]},{"cell_type":"markdown","metadata":{"id":"KXxHMnE3EeIL"},"source":["___"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}